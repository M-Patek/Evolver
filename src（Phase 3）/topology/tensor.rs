// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use std::collections::{HashMap, BTreeMap};
use rug::Integer;
use crate::core::affine::AffineTuple;
use blake3;
use serde::{Serialize, Deserialize};
use std::fs::File;
use std::io::{BufReader, BufWriter};

// [CONFIG]: å®‰å…¨æ€§ç¡¬é™åˆ¶
// å³ä½¿åœ¨æç«¯å†…å­˜å‹åŠ›ä¸‹ï¼Œä¹Ÿä¸å…è®¸å•ç‚¹å†å²æ— é™è†¨èƒ€
const MAX_TIMELINE_DEPTH: usize = 64; 
// å…¨å±€å®¹é‡è½¯ä¸Šé™ (Soft Limit)
const GLOBAL_CAPACITY_LIMIT: usize = 10_000_000;
// æ¯æ¬¡é©±é€çš„æ‰¹æ¬¡å¤§å°ï¼Œé¿å…é¢‘ç¹è§¦å‘
const EVICTION_BATCH_SIZE: usize = 100;

pub type Coordinate = Vec<usize>;

/// [Theoretical Best]: å¾®è§‚æ—¶é—´çº¿å®¹å™¨
/// å½“ç©ºé—´å‘ç”Ÿç¢°æ’æ—¶ï¼Œæˆ‘ä»¬åœ¨æ—¶é—´ç»´åº¦ä¸Šå±•å¼€ï¼Œä¿è¯é€»è¾‘çš„å› æœå®Œå¤‡æ€§ã€‚
#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct MicroTimeline {
    /// Key: Timestamp (Logic Sequence), Value: Affine Event
    /// BTreeMap ä¿è¯äº†æŒ‰æ—¶é—´æˆ³ä¸¥æ ¼æ’åºï¼Œè¿™å¯¹äºéäº¤æ¢ä»£æ•°è‡³å…³é‡è¦ã€‚
    pub events: BTreeMap<u64, AffineTuple>,
}

impl MicroTimeline {
    pub fn new() -> Self {
        MicroTimeline {
            events: BTreeMap::new(),
        }
    }

    /// [DoS Protection]: é™åˆ¶å•ç‚¹å†å²æ·±åº¦
    /// å¦‚æœä¸€ä¸ªåæ ‡ç§¯ç´¯äº†è¿‡å¤šçš„å†å²äº‹ä»¶ï¼ˆå¯èƒ½æ˜¯æ”»å‡»è€…åœ¨åˆ·çƒ­ç‚¹ï¼‰ï¼Œ
    /// æˆ‘ä»¬å¿…é¡»ä¿®å‰ªæœ€æ—§çš„äº‹ä»¶ä»¥é‡Šæ”¾å†…å­˜ã€‚
    pub fn prune(&mut self) {
        if self.events.len() > MAX_TIMELINE_DEPTH {
            // ä¿ç•™æœ€æ–°çš„ N ä¸ªï¼Œç§»é™¤æ—§çš„
            // è¿™æ˜¯ä¸€ä¸ª O(K) æ“ä½œï¼Œæ¯”æ— é™å¢é•¿å®‰å…¨å¾—å¤š
            let split_point = self.events.len().saturating_sub(MAX_TIMELINE_DEPTH);
            // æ‰¾åˆ°éœ€è¦ä¿ç•™çš„ç¬¬ä¸€ä¸ª key
            if let Some(&first_keep_key) = self.events.keys().nth(split_point) {
                // split_off è¿”å› >= key çš„éƒ¨åˆ†ï¼ˆå³æ–°çš„éƒ¨åˆ†ï¼‰ï¼Œæˆ‘ä»¬å°†æ—§çš„éƒ¨åˆ†ä¸¢å¼ƒ
                let keep = self.events.split_off(&first_keep_key);
                self.events = keep;
            }
        }
    }
}

#[derive(Serialize, Deserialize)]
pub struct HyperTensor {
    pub dimensions: usize,
    pub side_length: usize,
    pub discriminant: Integer,
    
    /// [Upgrade]: Data æ— è®ºæ˜¯ç©ºé—´è¿˜æ˜¯æ—¶é—´ï¼Œéƒ½æ˜¯æ­£äº¤çš„
    /// HashMap<Space, BTreeMap<Time, Event>>
    /// æ³¨æ„ï¼šä¸ºäº†çœŸæ­£çš„å¹¶å‘æ€§èƒ½ï¼Œæœªæ¥å»ºè®®å‡çº§ä¸º DashMap æˆ–åˆ†ç‰‡é”ç»“æ„ã€‚
    pub data: HashMap<Coordinate, MicroTimeline>,
    
    #[serde(skip)]
    pub cached_root: Option<AffineTuple>, 
}

impl HyperTensor {
    pub fn new(dim: usize, len: usize, discriminant: Integer) -> Self {
        HyperTensor {
            dimensions: dim,
            side_length: len,
            discriminant,
            data: HashMap::new(),
            cached_root: None,
        }
    }

    pub fn map_id_to_coord(&self, numeric_id: u64) -> Coordinate {
        let mut coord = Vec::with_capacity(self.dimensions);
        let mut temp = numeric_id;
        let l = self.side_length as u64;
        for _ in 0..self.dimensions {
            coord.push((temp % l) as usize);
            temp /= l;
        }
        coord
    }
    
    pub fn map_id_to_coord_hash(&self, user_id: &str) -> Coordinate {
        let mut hasher = blake3::Hasher::new();
        hasher.update(user_id.as_bytes());
        hasher.update(b":htp:coord:v3:orthogonal"); // Version Bump
        let hash_output = hasher.finalize();
        
        let mut coord = Vec::with_capacity(self.dimensions);
        let reader = hash_output.as_bytes();
        let l = self.side_length as u128;
        
        let mut val = u128::from_le_bytes(reader[0..16].try_into().unwrap());
        
        for _ in 0..self.dimensions {
            coord.push((val % l) as usize);
            val /= l;
        }
        coord
    }

    /// [FIXED]: å¼¹æ€§æ’å…¥ (Resilient Insertion)
    /// è§£å†³äº† DoS æ¼æ´ï¼šå½“å®¹é‡æ»¡æ—¶ï¼Œä¸å†æŠ¥é”™æ‹’ç»æœåŠ¡ï¼Œè€Œæ˜¯æ‰§è¡Œéšæœºé©±é€ (Random Eviction)ã€‚
    /// è¿™ä¿è¯äº†ç³»ç»Ÿåœ¨æ”»å‡»ä¸‹çš„å¯ç”¨æ€§ (Availability)ã€‚
    pub fn insert(&mut self, user_id: &str, new_tuple: AffineTuple, timestamp: u64) -> Result<(), String> {
        // [DoS Defense 1]: å…¨å±€å®¹é‡æ£€æŸ¥ä¸ç´§æ€¥é©±é€
        if self.data.len() >= GLOBAL_CAPACITY_LIMIT {
            self.perform_emergency_eviction();
        }

        let coord = self.map_id_to_coord_hash(user_id);
        
        // è·å–æˆ–åˆ›å»ºå¾®è§‚æ—¶é—´çº¿
        let timeline = self.data.entry(coord).or_insert_with(MicroTimeline::new);
        
        // [DoS Defense 2]: å•ç‚¹æ·±åº¦ä¿®å‰ª
        // é˜²æ­¢æ”»å‡»è€…ç›¯ç€ä¸€ä¸ªåæ ‡æ— é™å†™å…¥
        timeline.prune();
        
        timeline.events.insert(timestamp, new_tuple);

        self.cached_root = None;
        Ok(())
    }

    /// ğŸ§¹ ç´§æ€¥é©±é€ç­–ç•¥ (Emergency Eviction Strategy)
    /// å½“ç³»ç»Ÿè¿‡è½½æ—¶ï¼Œéšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†æ•°æ®ä»¥è…¾å‡ºç©ºé—´ã€‚
    /// ç›¸æ¯”äº LRUï¼Œéšæœºé©±é€åœ¨ HashMap ä¸Šæ˜¯ O(1) çš„ï¼Œæ›´é€‚åˆæŠ— DoSã€‚
    fn perform_emergency_eviction(&mut self) {
        // ç”±äº Rust HashMap çš„è¿­ä»£é¡ºåºæ˜¯ä¸ç¡®å®šçš„ï¼ˆåŸºäº Hash ç§å­ï¼‰ï¼Œ
        // ç›´æ¥å– iter().next() å°±ç­‰åŒäºä¼ªéšæœºé€‰æ‹©ã€‚
        // æˆ‘ä»¬æ‰¹é‡ç§»é™¤ key ä»¥å‡å°‘ rehashing å¼€é”€ã€‚
        
        let keys_to_remove: Vec<Coordinate> = self.data.keys()
            .take(EVICTION_BATCH_SIZE)
            .cloned()
            .collect();

        for k in keys_to_remove {
            self.data.remove(&k);
        }
        
        // log::warn!("âš ï¸ HyperTensor Capacity Limit Reached. Evicted {} entries.", EVICTION_BATCH_SIZE);
    }
    
    pub fn save_to_disk(&self, path: &str) -> Result<(), String> {
        let file = File::create(path).map_err(|e| e.to_string())?;
        let writer = BufWriter::new(file);
        bincode::serialize_into(writer, self).map_err(|e| e.to_string())?;
        Ok(())
    }

    pub fn load_from_disk(path: &str) -> Result<Self, String> {
        let file = File::open(path).map_err(|e| e.to_string())?;
        let reader = BufReader::new(file);
        let tensor: HyperTensor = bincode::deserialize_from(reader).map_err(|e| e.to_string())?;
        Ok(tensor)
    }

    // è¾…åŠ©ï¼šè·å–æŸä¸ªåæ ‡çš„èšåˆçŠ¶æ€ï¼ˆç”¨äº Proof ç”Ÿæˆç­‰ï¼‰
    pub fn get_collapsed_state(&self, coord: &Coordinate) -> Result<AffineTuple, String> {
        if let Some(timeline) = self.data.get(coord) {
            let mut agg = AffineTuple::identity(&self.discriminant);
            for tuple in timeline.events.values() {
                agg = agg.compose(tuple, &self.discriminant)?;
            }
            Ok(agg)
        } else {
            Ok(AffineTuple::identity(&self.discriminant))
        }
    }

    pub fn get_segment_tree_path(&self, coord: &Coordinate, _axis: usize) -> Vec<AffineTuple> {
        let mut path = Vec::new();
        // è¿™é‡Œéœ€è¦è¿”å›èšåˆåçš„çŠ¶æ€ä½œä¸ºå¶å­èŠ‚ç‚¹
        if let Ok(t) = self.get_collapsed_state(coord) {
            path.push(t);
        } else {
            // Error fallback
            path.push(AffineTuple::identity(&self.discriminant));
        }
        
        if self.side_length > 1 {
             path.push(AffineTuple::identity(&self.discriminant));
        }
        path
    }
}
